---
layout: post
title: Objectivity
---

**If all data is interpreted, as Kurgan (and some of you) say, what are datasets really tell us? How do we know when and how we can trust a dataset, and is there a standard rule for this?**

If all data is interpreted, or is a visualization as Kurgan says, then we should think of data as a projection of the real world through a collection of lenses. These lenses are warped and never depict the world as it truly is. These lenses or methods of data collections may select, blur, and magnify information. In order to have a level of trust in a dataset we must gain insight into the methods of data collection. The examination of metadata helps us give viewers some gauge of the accuracy of the data, but can never prove it's truthfulness for certain. As a result there's no standard rule for evaluating the accuracty of a dataset. Still this doesn't mean we can't achieve a level of confidence that lets us make use of data.

**What are the different types of truth that a dataset can have? Can data reflect a number of different realities at once?**

As Daston and Galison highlight, there are different ways we can translate reality onto the page as a visual figure. For example a plant could be depicted with a mechanical objectivity or Truth-to-Nature approach. One of these approaches isn't necessarily more accurate than the other. They serve different purposes. The mechanical approach may better represent a specific existing plant. On the other hand, the Truth-to-Nature approach may better accurately depict the ideal or most average plant of that category.

While we have these different approaches available to represent individual specimens, I don't think they apply as much to full datasets. A useful and representative dataset should contain many examples recorded consistently. Otherwise truthful conclusions cannot be extracted from the data.
